{
  "alpaca13b": 13000000000,
  "falcon40b": 40000000000,
  "tiiuae-falcon-7b-instruct": 7000000000,
  "gpt4-x-alpaca": 13000000000,
  "llama13b": 13000000000,
  "llama65b": 65000000000,
  "gpt2-xl": 1500000000,
  "gpt2": 117000000,
  "gpt2-medium": 345000000,
  "gpt2-large": 774000000
}
