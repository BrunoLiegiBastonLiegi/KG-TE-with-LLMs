{
  "alpaca-13b": 13000000000,
  "falcon-40b-instruct": 40000000000,
  "falcon-7b-instruct": 7000000000,
  "falcon-40b": 40000000000,
  "falcon-7b": 7000000000,
  "gpt4-x-alpaca": 13000000000,
  "llama-13b": 13000000000,
  "llama-65b": 65000000000,
  "gpt2-xl": 1500000000,
  "gpt2": 117000000,
  "gpt2-medium": 345000000,
  "gpt2-large": 774000000,
  "gpt3.5-turbo.conf": 175000000000,
  "gpt4": 1000000000000
}
