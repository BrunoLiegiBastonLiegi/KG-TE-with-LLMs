{
  "gpt2 (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.15544734078105604,
    "R": 0.20118635689569947,
    "F1": 0.17538378669539453
  },
  "gpt2-xl (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.16060655470406,
    "R": 0.24345032130499258,
    "F1": 0.1935357107770901
  },
  "falcon-7b (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.18432827034028024,
    "R": 0.38692535837864556,
    "F1": 0.24970093308876307
  },
  "falcon-40b (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.16107241975179348,
    "R": 0.3801285219970341,
    "F1": 0.22626797601971388
  },
  "llama-13b (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.18586387434554974,
    "R": 0.3685121107266436,
    "F1": 0.24709976798143848
  },
  "llama-65b (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.17706739216686504,
    "R": 0.3860603064755314,
    "F1": 0.2427822032251797
  },
  "gpt-3.5-turbo (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.06069377655352724,
    "R": 0.1606524962926347,
    "F1": 0.08810274135068281
  },
  "gpt-4 (KB top-5) T=0.1 prompt=base_no_exaples": {
    "P": 0.06680898549318325,
    "R": 0.17016806722689076,
    "F1": 0.0959481587290527
  }
}